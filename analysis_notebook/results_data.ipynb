{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4081139-7ba5-4e33-9f50-c2096e76acfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fairlearn.metrics import equalized_odds_difference, demographic_parity_difference\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from scipy.stats import t\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e94bb47-ea80-46bf-a4c9-67840d48baa9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Metrics (Mean Accuracy, 95% CI, f1-score, EO, DP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc8082f-0bc1-4fd0-aa9d-fd0af299956b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## All data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51968d02-7fc1-4e47-9640-efef01e1b4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ./results_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "def process_file(file_path):\n",
    "    \"\"\"\n",
    "    Processes a single CSV file to compute mean accuracy, CI, F1-score, and fairness metrics.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing mean metrics for the file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        data = data[data['dataset_split'] == 'test']\n",
    "\n",
    "        required_columns = {'dataset_split', 'ground_truth'}\n",
    "        if not required_columns.issubset(data.columns):\n",
    "            raise ValueError(f\"{file_path} is missing required columns: {required_columns - set(data.columns)}\")\n",
    "\n",
    "        true_labels = data['ground_truth']\n",
    "        sensitive_features = [col for col in data.columns if col in ['gender', 'race', 'age']]\n",
    "        prediction_columns = [col for col in data.columns if \"predicted_label\" in col]\n",
    "\n",
    "        def accuracy_with_ci(y_true, y_pred, confidence=0.95):\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            n = len(y_true)\n",
    "            se = np.sqrt(accuracy * (1 - accuracy) / n)\n",
    "            h = se * t.ppf((1 + confidence) / 2, n - 1)\n",
    "            return accuracy, (accuracy - h, accuracy + h)\n",
    "\n",
    "        def binarize_labels(labels):\n",
    "            unique_labels = labels.unique()\n",
    "            return labels.apply(lambda x: 1 if x == unique_labels[0] else 0)\n",
    "\n",
    "        results = []\n",
    "        for pred_col in prediction_columns:\n",
    "            predictions = data[pred_col]\n",
    "            accuracy, ci = accuracy_with_ci(true_labels, predictions)\n",
    "            f1_micro = f1_score(true_labels, predictions, average='micro')\n",
    "\n",
    "            true_labels_binary = binarize_labels(true_labels)\n",
    "            predictions_binary = binarize_labels(predictions)\n",
    "\n",
    "            fairness_metrics = {}\n",
    "            for sensitive_feature in sensitive_features:\n",
    "                eod = equalized_odds_difference(\n",
    "                    y_true=true_labels_binary,\n",
    "                    y_pred=predictions_binary,\n",
    "                    sensitive_features=data[sensitive_feature]\n",
    "                )\n",
    "                dpd = demographic_parity_difference(\n",
    "                    y_true=true_labels_binary,\n",
    "                    y_pred=predictions_binary,\n",
    "                    sensitive_features=data[sensitive_feature]\n",
    "                )\n",
    "                fairness_metrics[f\"EO_{sensitive_feature}\"] = eod\n",
    "                fairness_metrics[f\"DP_{sensitive_feature}\"] = dpd\n",
    "\n",
    "            results.append({\n",
    "                'Prediction Column': pred_col,\n",
    "                'Accuracy': accuracy,\n",
    "                '95% CI Lower': ci[0],\n",
    "                '95% CI Upper': ci[1],\n",
    "                'F1-Score': f1_micro,\n",
    "                **fairness_metrics\n",
    "            })\n",
    "\n",
    "        results_df = pd.DataFrame(results)\n",
    "        fixed_columns = ['Prediction Column', 'Accuracy', '95% CI Lower', '95% CI Upper', 'F1-Score']\n",
    "        metrics_order = [f\"EO_{feat}\" for feat in sensitive_features] + [f\"DP_{feat}\" for feat in sensitive_features]\n",
    "        sorted_columns = fixed_columns + metrics_order\n",
    "        results_df = results_df[sorted_columns]\n",
    "\n",
    "        mean_results = results_df.mean(numeric_only=True).to_frame(name='Mean').T\n",
    "\n",
    "        # Convert specific metrics to percentage and apply appropriate rounding\n",
    "        percentage_metrics = ['Accuracy', '95% CI Lower', '95% CI Upper']\n",
    "        mean_results[percentage_metrics] = mean_results[percentage_metrics] * 100\n",
    "        mean_results[percentage_metrics] = mean_results[percentage_metrics].round(2)\n",
    "\n",
    "        # Round other metrics to 3 decimal places\n",
    "        other_metrics = list(set(mean_results.columns) - set(percentage_metrics))\n",
    "        mean_results[other_metrics] = mean_results[other_metrics].round(3)\n",
    "\n",
    "        mean_results.index = [os.path.basename(file_path)]\n",
    "\n",
    "        return mean_results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Define file paths\n",
    "file_paths = [\n",
    "    \"vit.csv\",\n",
    "    \"vit_aug.csv\",\n",
    "    \"cnn.csv\", \n",
    "    \"cnn_aug.csv\", \n",
    "    \"vgg.csv\", \n",
    "    \"vgg_aug.csv\", \n",
    "    \"vit_aug_under.csv\",\n",
    "    \"vit_elastic.csv\", \n",
    "    \"vit_elastic_under.csv\",\n",
    "    \"t1.csv\", \n",
    "    \"t2.csv\", \n",
    "    \"t3(s1).csv\", \n",
    "    \"t4.csv\",\n",
    "    \"t5.csv\", \n",
    "    \"t6.csv\", \n",
    "    \"s2.csv\", \n",
    "    \"s3.csv\", \n",
    "    \"s4.csv\", \n",
    "    \"s5.csv\",\n",
    "    \"a1.csv\",\n",
    "    \"a2.csv\",\n",
    "    \"a3.csv\",\n",
    "    \"a4.csv\"\n",
    "]\n",
    "\n",
    "# Process all files and collect results\n",
    "all_results = []\n",
    "for file_path in file_paths:\n",
    "    result = process_file(file_path)\n",
    "    if result is not None:\n",
    "        all_results.append(result)\n",
    "\n",
    "# Combine all results into a single DataFrame\n",
    "final_results_df = pd.concat(all_results)\n",
    "\n",
    "# Save to CSV\n",
    "output_file = f\"./results_metrics.csv\"\n",
    "final_results_df.to_csv(output_file)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15e6100-b2a5-419c-8fe7-fa1d2d20e146",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Underrepresented groups samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfbac3c3-6f28-4513-aa97-d86315fe3be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ./results_metrics_under.csv\n"
     ]
    }
   ],
   "source": [
    "def process_file(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        data = data[data['dataset_split'] == 'test']\n",
    "\n",
    "        required_columns = {'dataset_split', 'ground_truth', 'gender', 'race', 'age'}\n",
    "        if not required_columns.issubset(data.columns):\n",
    "            raise ValueError(f\"{file_path} is missing required columns: {required_columns - set(data.columns)}\")\n",
    "\n",
    "        # Define minority group conditions\n",
    "        def is_minority(row):\n",
    "            return (\n",
    "                (row['gender'] == 0 and row['ground_truth'] in [1, 2, 5]) or\n",
    "                (row['race'] == 1 and row['ground_truth'] in [1, 2, 5]) or\n",
    "                (row['age'] in [0, 4] and row['ground_truth'] in [1, 2, 5])\n",
    "            )\n",
    "\n",
    "        data['is_minority'] = data.apply(is_minority, axis=1)\n",
    "        minority_data = data[data['is_minority']]\n",
    "\n",
    "        true_labels_minority = minority_data['ground_truth']\n",
    "        true_labels_full = data['ground_truth']\n",
    "        sensitive_features = [col for col in data.columns if col in ['gender', 'race', 'age']]\n",
    "        prediction_columns = [col for col in data.columns if \"predicted_label\" in col]\n",
    "\n",
    "        def accuracy_with_ci(y_true, y_pred, confidence=0.95):\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            n = len(y_true)\n",
    "            se = np.sqrt(accuracy * (1 - accuracy) / n)\n",
    "            h = se * t.ppf((1 + confidence) / 2, n - 1)\n",
    "            return accuracy, (accuracy - h, accuracy + h)\n",
    "\n",
    "        def binarize_labels(labels):\n",
    "            unique_labels = labels.unique()\n",
    "            return labels.apply(lambda x: 1 if x == unique_labels[0] else 0)\n",
    "\n",
    "        results = []\n",
    "        for pred_col in prediction_columns:\n",
    "            # Metrics for minority group\n",
    "            predictions_minority = minority_data[pred_col]\n",
    "            accuracy_minority, ci_minority = accuracy_with_ci(true_labels_minority, predictions_minority)\n",
    "            f1_micro_minority = f1_score(true_labels_minority, predictions_minority, average='micro')\n",
    "\n",
    "            # Fairness metrics for the full dataset\n",
    "            predictions_full = data[pred_col]\n",
    "            true_labels_binary_full = binarize_labels(true_labels_full)\n",
    "            predictions_binary_full = binarize_labels(predictions_full)\n",
    "\n",
    "            fairness_metrics = {}\n",
    "            for sensitive_feature in sensitive_features:\n",
    "                eod = equalized_odds_difference(\n",
    "                    y_true=true_labels_binary_full,\n",
    "                    y_pred=predictions_binary_full,\n",
    "                    sensitive_features=data[sensitive_feature]\n",
    "                )\n",
    "                dpd = demographic_parity_difference(\n",
    "                    y_true=true_labels_binary_full,\n",
    "                    y_pred=predictions_binary_full,\n",
    "                    sensitive_features=data[sensitive_feature]\n",
    "                )\n",
    "                fairness_metrics[f\"EO_{sensitive_feature}\"] = eod\n",
    "                fairness_metrics[f\"DP_{sensitive_feature}\"] = dpd\n",
    "\n",
    "            results.append({\n",
    "                'Prediction Column': pred_col,\n",
    "                'Accuracy': accuracy_minority,\n",
    "                '95% CI Lower': ci_minority[0],\n",
    "                '95% CI Upper': ci_minority[1],\n",
    "                'F1-Score': f1_micro_minority,\n",
    "                **fairness_metrics\n",
    "            })\n",
    "\n",
    "        results_df = pd.DataFrame(results)\n",
    "        fixed_columns = ['Prediction Column', 'Accuracy', '95% CI Lower', \n",
    "                         '95% CI Upper', 'F1-Score']\n",
    "        metrics_order = [f\"EO_{feat}\" for feat in sensitive_features] + [f\"DP_{feat}\" for feat in sensitive_features]\n",
    "        sorted_columns = fixed_columns + metrics_order\n",
    "        results_df = results_df[sorted_columns]\n",
    "\n",
    "        mean_results = results_df.mean(numeric_only=True).to_frame(name='Mean').T\n",
    "\n",
    "        # Convert specific metrics to percentage and apply appropriate rounding\n",
    "        percentage_metrics = ['Accuracy', '95% CI Lower', '95% CI Upper']\n",
    "        mean_results[percentage_metrics] = mean_results[percentage_metrics] * 100\n",
    "        mean_results[percentage_metrics] = mean_results[percentage_metrics].round(2)\n",
    "\n",
    "        # Round other metrics to 3 decimal places\n",
    "        other_metrics = list(set(mean_results.columns) - set(percentage_metrics))\n",
    "        mean_results[other_metrics] = mean_results[other_metrics].round(3)\n",
    "\n",
    "        mean_results.index = [os.path.basename(file_path)]\n",
    "\n",
    "        return mean_results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Define file paths\n",
    "file_paths = [\n",
    "    \"vit.csv\",\n",
    "    \"vit_aug.csv\",\n",
    "    \"cnn.csv\", \n",
    "    \"cnn_aug.csv\", \n",
    "    \"vgg.csv\", \n",
    "    \"vgg_aug.csv\", \n",
    "    \"vit_aug_under.csv\",\n",
    "    \"vit_elastic.csv\", \n",
    "    \"vit_elastic_under.csv\",\n",
    "    \"t1.csv\", \n",
    "    \"t2.csv\", \n",
    "    \"t3(s1).csv\", \n",
    "    \"t4.csv\",\n",
    "    \"t5.csv\", \n",
    "    \"t6.csv\", \n",
    "    \"s2.csv\", \n",
    "    \"s3.csv\", \n",
    "    \"s4.csv\", \n",
    "    \"s5.csv\",\n",
    "    \"a1.csv\",\n",
    "    \"a2.csv\",\n",
    "    \"a3.csv\",\n",
    "    \"a4.csv\"\n",
    "]\n",
    "\n",
    "# Process all files and collect results\n",
    "all_results = []\n",
    "for file_path in file_paths:\n",
    "    result = process_file(file_path)\n",
    "    if result is not None:\n",
    "        all_results.append(result)\n",
    "\n",
    "# Combine all results into a single DataFrame\n",
    "final_results_df = pd.concat(all_results)\n",
    "\n",
    "# Save to CSV\n",
    "output_file = f\"./results_metrics_under.csv\"\n",
    "final_results_df.to_csv(output_file)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc761e1b-6471-4228-ae11-ae19a974a46b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Emotion Class-wise Results with Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5ba6b82-bb8e-47c0-b978-99f037e34bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ./vit_class_wise.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = \"./vit.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "data = data[data['dataset_split'] == 'test']\n",
    "\n",
    "# Extract true labels, sensitive features, and prediction columns\n",
    "true_labels = data['ground_truth']\n",
    "sensitive_features = ['gender', 'race', 'age']\n",
    "prediction_columns = [col for col in data.columns if \"predicted_label\" in col]\n",
    "\n",
    "# Map class numbers to emotion labels\n",
    "emotion_mapping = {\n",
    "    0: \"Surprise\",\n",
    "    1: \"Fear\",\n",
    "    2: \"Disgust\",\n",
    "    3: \"Happiness\",\n",
    "    4: \"Sadness\",\n",
    "    5: \"Anger\",\n",
    "    6: \"Neutral\"\n",
    "}\n",
    "\n",
    "# Function to calculate accuracy with t-based CI\n",
    "def accuracy_with_ci(y_true, y_pred, confidence=0.95):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    n = len(y_true)\n",
    "    se = np.sqrt(accuracy * (1 - accuracy) / n)  # Standard error\n",
    "    h = se * t.ppf((1 + confidence) / 2, n - 1)  # Margin of error\n",
    "    return accuracy, (accuracy - h, accuracy + h)\n",
    "\n",
    "# Initialize results storage for overall metrics\n",
    "overall_results = []\n",
    "\n",
    "# Loop through each prediction column\n",
    "for pred_col in prediction_columns:\n",
    "    predictions = data[pred_col]\n",
    "\n",
    "    # Calculate overall accuracy and CI\n",
    "    accuracy, ci = accuracy_with_ci(true_labels, predictions)\n",
    "    f1_micro = f1_score(true_labels, predictions, average='micro')  # Multi-class F1-score\n",
    "\n",
    "    # Initialize storage for fairness metrics\n",
    "    fairness_metrics = {}\n",
    "\n",
    "    # Calculate fairness metrics for each sensitive feature\n",
    "    for sensitive_feature in sensitive_features:\n",
    "        for class_num in emotion_mapping.keys():\n",
    "            true_labels_binary = (true_labels == class_num).astype(int)\n",
    "            predictions_binary = (predictions == class_num).astype(int)\n",
    "\n",
    "            fairness_metrics[f\"EO_{sensitive_feature}_class{class_num}\"] = equalized_odds_difference(\n",
    "                y_true=true_labels_binary,\n",
    "                y_pred=predictions_binary,\n",
    "                sensitive_features=data[sensitive_feature]\n",
    "            )\n",
    "            fairness_metrics[f\"DP_{sensitive_feature}_class{class_num}\"] = demographic_parity_difference(\n",
    "                y_true=true_labels_binary,\n",
    "                y_pred=predictions_binary,\n",
    "                sensitive_features=data[sensitive_feature]\n",
    "            )\n",
    "\n",
    "    # Store overall results\n",
    "    overall_results.append({\n",
    "        'Prediction Column': pred_col,\n",
    "        'Accuracy': accuracy * 100,\n",
    "        '95% CI Lower': ci[0] * 100,\n",
    "        '95% CI Upper': ci[1] * 100,\n",
    "        'F1-Score': f1_micro * 100,\n",
    "        **fairness_metrics\n",
    "    })\n",
    "\n",
    "# Convert overall results to a DataFrame\n",
    "overall_results_df = pd.DataFrame(overall_results)\n",
    "\n",
    "# Initialize storage for class-wise metrics\n",
    "classwise_results = []\n",
    "\n",
    "# Loop through each emotion class\n",
    "for class_num, emotion in emotion_mapping.items():\n",
    "    true_labels_binary = (true_labels == class_num).astype(int)\n",
    "\n",
    "    accuracies, ci_lowers, ci_uppers, f1_scores = [], [], [], []\n",
    "\n",
    "    for pred_col in prediction_columns:\n",
    "        predictions_binary = (data[pred_col] == class_num).astype(int)\n",
    "        \n",
    "        accuracy, ci = accuracy_with_ci(true_labels_binary, predictions_binary)\n",
    "        accuracies.append(accuracy * 100)\n",
    "        ci_lowers.append(ci[0] * 100)\n",
    "        ci_uppers.append(ci[1] * 100)\n",
    "        f1 = f1_score(true_labels_binary, predictions_binary, average='binary', pos_label=1)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    mean_ci_lower = np.mean(ci_lowers)\n",
    "    mean_ci_upper = np.mean(ci_uppers)\n",
    "    mean_f1_score = np.mean(f1_scores)\n",
    "\n",
    "    eo_gender = overall_results_df[f\"EO_gender_class{class_num}\"].mean()\n",
    "    eo_race = overall_results_df[f\"EO_race_class{class_num}\"].mean()\n",
    "    eo_age = overall_results_df[f\"EO_age_class{class_num}\"].mean()\n",
    "    dp_gender = overall_results_df[f\"DP_gender_class{class_num}\"].mean()\n",
    "    dp_race = overall_results_df[f\"DP_race_class{class_num}\"].mean()\n",
    "    dp_age = overall_results_df[f\"DP_age_class{class_num}\"].mean()\n",
    "\n",
    "    classwise_results.append({\n",
    "        \"Emotion Class\": emotion,\n",
    "        \"Mean Accuracy (%)\": round(mean_accuracy, 2),\n",
    "        \"CI Lower (%)\": round(mean_ci_lower, 2),\n",
    "        \"CI Upper (%)\": round(mean_ci_upper, 2),\n",
    "        \"F1-Score\": round(mean_f1_score, 3),\n",
    "        \"EO_Gender\": round(eo_gender, 3),\n",
    "        \"EO_Race\": round(eo_race, 3),\n",
    "        \"EO_Age\": round(eo_age, 3),\n",
    "        \"DP_Gender\": round(dp_gender, 3),\n",
    "        \"DP_Race\": round(dp_race, 3),\n",
    "        \"DP_Age\": round(dp_age, 3),\n",
    "    })\n",
    "\n",
    "# Convert class-wise results to a DataFrame\n",
    "classwise_results_df = pd.DataFrame(classwise_results)\n",
    "\n",
    "# Reorder columns\n",
    "classwise_results_df = classwise_results_df[[\n",
    "    \"Emotion Class\", \"Mean Accuracy (%)\", \"CI Lower (%)\", \"CI Upper (%)\", \"F1-Score\",\n",
    "    \"EO_Gender\", \"EO_Race\", \"EO_Age\",\n",
    "    \"DP_Gender\", \"DP_Race\", \"DP_Age\"\n",
    "]]\n",
    "\n",
    "output_path = \"./vit_class_wise.csv\"\n",
    "classwise_results_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a811d09-b403-443e-90e3-c1f731e4c2f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Balanced Score of Accuracy and Fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d74b1fa7-ba7c-4dcc-8d84-41deffadf029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced scores saved to ./balanced_scores_under.csv\n"
     ]
    }
   ],
   "source": [
    "def calculate_balanced_scores_from_csv(input_file, output_file, beta1=0.5, beta2=0.5):\n",
    "    \"\"\"\n",
    "    Extract mean metrics from the CSV file, calculate balanced scores, and save the required columns.\n",
    "\n",
    "    Parameters:\n",
    "        input_file (str): Path to the CSV file containing mean results.\n",
    "        output_file (str): Path to save the results with balanced scores.\n",
    "        beta1 (float): Weight for fairness penalty in the balanced score.\n",
    "        beta2 (float): Weight for mean accuracy in the balanced score.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with only the balanced scores and file names.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the CSV file\n",
    "        data = pd.read_csv(input_file)\n",
    "\n",
    "        # Extract required columns for balanced score calculation\n",
    "        required_columns = [\n",
    "            'Accuracy', 'EO_gender', 'EO_race', 'EO_age',\n",
    "            'DP_gender', 'DP_race', 'DP_age'\n",
    "        ]\n",
    "        if not set(required_columns).issubset(data.columns):\n",
    "            raise ValueError(f\"The input file is missing required columns: {set(required_columns) - set(data.columns)}\")\n",
    "\n",
    "        # Calculate balanced scores for each fairness metric\n",
    "        balanced_columns = []\n",
    "        for fairness_metric in ['EO_gender', 'EO_race', 'EO_age', 'DP_gender', 'DP_race', 'DP_age']:\n",
    "            balanced_col = f\"Balanced_{fairness_metric}\"\n",
    "            data[balanced_col] = (\n",
    "                beta1 * (1 - data[fairness_metric]) + beta2 * data['Accuracy']/100\n",
    "            )\n",
    "            balanced_columns.append(balanced_col)\n",
    "\n",
    "        # Retain only balanced score columns and file name\n",
    "        data = data[['Unnamed: 0'] + balanced_columns]\n",
    "\n",
    "        # Rename 'Unnamed: 0' to 'File Name' for clarity\n",
    "        data.rename(columns={'Unnamed: 0': 'File Name'}, inplace=True)\n",
    "\n",
    "        # Round results to 3 decimal places\n",
    "        data[balanced_columns] = data[balanced_columns].round(3)\n",
    "\n",
    "        # Save the final DataFrame to a CSV\n",
    "        data.to_csv(output_file, index=False)\n",
    "        print(f\"Balanced scores saved to {output_file}\")\n",
    "\n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {input_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Define input and output file paths\n",
    "input_csv_file = \"./results_metrics_under.csv\"\n",
    "output_csv_file = \"./balanced_scores_under.csv\"\n",
    "\n",
    "# Calculate balanced scores and save the required columns\n",
    "final_balanced_scores = calculate_balanced_scores_from_csv(input_csv_file, output_csv_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
